{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWbSEih-W0uv"
   },
   "source": [
    "# Deep Q-Network implementation\n",
    "\n",
    "based on:\n",
    "https://github.com/yandexdataschool/Practical_RL/tree/master/week4_approx_rl\n",
    "\n",
    "reference:\n",
    "https://github.com/BoYanSTKO/Practical_RL-coursera/blob/master/week4_approx/dqn_atari.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "t7iNi9HqXgUC",
    "outputId": "9af24e04-86ff-4bac-baa0-34ee9bd45240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update \n",
    "!apt install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
    "\n",
    "!pip3 install pyvirtualdisplay piglet gym torch torchvision torchsummary\n",
    "!pip3 install opencv-python\n",
    "!pip3 install \"gym[atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XzrJs-nW0u1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "import gym\n",
    "from gym.core import Wrapper\n",
    "\n",
    "# from scipy.misc import imresize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhcBF0eNq1NK"
   },
   "source": [
    "# Descriptions of each class and function\n",
    "\n",
    "* **PrepsocessAtari**: crop, resize, grayscale the image\n",
    "* **[FrameBuffer](https://github.com/yandexdataschool/Practical_RL/blob/master/week4_approx_rl/framebuffer.py)**: A sym wrapper that output N frames observation\n",
    "\n",
    "* **make_env()**: apply PreprocessAtari and FrameBuffer to the environment.\n",
    "\n",
    "After env = make_env(), the env returns obs, reward, done, info;\n",
    "1. obs: 4x84x84 ndarray\n",
    "2. reward: float32\n",
    "3. done: bool\n",
    "\n",
    "---\n",
    "\n",
    "* **[ReplayBuffer](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py)**: store and sample \"obs_t, action, reward, obs_tp1, done\". \n",
    "\n",
    "ReplayBuffer.sample(batch_size) returns:\n",
    "1. np.array(obses_t):     batch_size x4x84x84\n",
    "2. np.array(actions):      batch_size\n",
    "3. np.array(rewards):     batch_size\n",
    "4. np.array(obses_tp1): batch_size x4x84x84\n",
    "5. np.array(dones):         batch_size\n",
    "\n",
    "\n",
    "* **play_and_record()**:    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. Returns sum of rewards over time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "779IdOxNaSyK"
   },
   "outputs": [],
   "source": [
    "class PreprocessAtari(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self,env)\n",
    "        \n",
    "        self.img_size = (84, 84)\n",
    "        self.observation_space = Box(0.0, 1.0, (1, self.img_size[0], self.img_size[1]))\n",
    "\n",
    "    def _observation(self, img):\n",
    "        img = img[34:-16, 8:-8, :]\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        img = img.mean(-1, keepdims=True)  # grayscale\n",
    "        img = img.astype('float32') / 255.\n",
    "               \n",
    "        return img.transpose([2, 0, 1])\n",
    "      \n",
    "\n",
    "class FrameBuffer(Wrapper):\n",
    "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
    "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
    "        super(FrameBuffer, self).__init__(env)\n",
    "        self.dim_order = dim_order\n",
    "        if dim_order == 'tensorflow':\n",
    "            height, width, n_channels = env.observation_space.shape\n",
    "            obs_shape = [height, width, n_channels * n_frames]\n",
    "        elif dim_order == 'pytorch':\n",
    "            n_channels, height, width = env.observation_space.shape\n",
    "            obs_shape = [n_channels * n_frames, height, width]\n",
    "        else:\n",
    "            raise ValueError('dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
    "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
    "        self.update_buffer(self.env.reset())\n",
    "        return self.framebuffer\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
    "        new_img, reward, done, info = self.env.step(action)\n",
    "        self.update_buffer(new_img)\n",
    "        return self.framebuffer, reward, done, info\n",
    "    \n",
    "    def update_buffer(self, img):\n",
    "        if self.dim_order == 'tensorflow':\n",
    "            offset = self.env.observation_space.shape[-1]\n",
    "            axis = -1\n",
    "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
    "        elif self.dim_order == 'pytorch':\n",
    "            offset = self.env.observation_space.shape[0]\n",
    "            axis = 0\n",
    "            cropped_framebuffer = self.framebuffer[:-offset]\n",
    "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
    "\n",
    "        \n",
    "def make_env():\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    env = PreprocessAtari(env)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n",
    "  \n",
    "  \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "      \n",
    "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer.\n",
    "    :returns: return sum of rewards over time\n",
    "    \"\"\"\n",
    "    # Make sure that the state is only one batch state, 4x84x84\n",
    "    # State at the beginning of rollout\n",
    "    s = env.framebuffer\n",
    "    R = 0.0\n",
    "    \n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for t in range(n_steps):\n",
    "        qvalues = agent.get_qvalues(torch.tensor(s).unsqueeze(0))\n",
    "        action = agent.sample_actions(qvalues).item()\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "        exp_replay.add(s, action, r, next_s, done=done)\n",
    "        if done == True:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "    return R       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "RcfkP67VW0vF",
    "outputId": "d0f9e79c-2569-4a53-dadc-e826c9a7f07a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: <class '__main__.PreprocessAtari'> doesn't implement 'observation' method. Maybe it implements deprecated '_observation' method.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "unL-xNVGW0vJ",
    "outputId": "7e024bda-079e-4400-e56c-08e8c702a026"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMsAAAEHCAYAAAAAprJIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni0gghUUk0P8DQjUQnlSHuewAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b403e8160>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAACCCAYAAACJkUn8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b403b87f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "\n",
    "plt.title(\"Game image\")\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "plt.show()\n",
    "plt.title(\"Agent observation (4 frames left to right)\")\n",
    "plt.imshow(obs.transpose([1,0,2]).reshape([obs.shape[1],-1]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyy-VZpkvSJo"
   },
   "source": [
    "# How the DQN flows\n",
    "\n",
    "The flow is as follows:\n",
    "\n",
    "see: https://drive.google.com/file/d/1mJKZEf4laDnPRNSrPesXzlHu36ibImcv/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3yi78HrW0vS"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # input obs, output n_actions\n",
    "        # The network is based on \"Mnih, 2015\"\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.l1 = nn.Linear(64*7*7, 512)\n",
    "        self.l2 = nn.Linear(512, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "      \n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0, reuse=False):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        \n",
    "        self.dqn = DQN(n_actions)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"takes agent's observation, returns qvalues. \"\"\"\n",
    "        qvalues = self.dqn(states)\n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues_for_actions(self, qvalues, actions):\n",
    "        return qvalues.gather(1, \\\n",
    "                actions.unsqueeze(0).transpose(0, 1)).squeeze()\n",
    "   \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = torch.tensor(np.random.choice(n_actions, size=batch_size))\n",
    "        best_actions = qvalues.argmax(1)\n",
    "        should_explore = torch.tensor(np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])).byte()\n",
    "        return torch.where(should_explore, random_actions, best_actions)\n",
    "\n",
    "      \n",
    "def optimize(current_action_qvalues, optimizer, target_dqn, \\\n",
    "             reward_batch, next_obs_batch, is_done_batch):\n",
    "    next_qvalues_target = target_dqn.get_qvalues(next_obs_batch)\n",
    "\n",
    "    # compute state values by taking max over next_qvalues_target for all actions\n",
    "    next_state_values_target = next_qvalues_target.max(1)[0]\n",
    "    next_state_values_target = torch.where(torch.tensor(is_done_batch).byte(), \\\n",
    "                                 torch.tensor(reward_batch), \\\n",
    "                                 torch.tensor(next_state_values_target))\n",
    "    \n",
    "    # compute Q_reference(s,a) as per formula above.\n",
    "    reference_qvalues = reward_batch + gamma * next_state_values_target\n",
    "\n",
    "    # Define loss function for sgd.\n",
    "    td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
    "    td_loss = torch.mean(td_loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    td_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return td_loss.item()\n",
    "  \n",
    "      \n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            s = torch.tensor(s).unsqueeze(0)\n",
    "            qvalues = agent.get_qvalues(s)\n",
    "            action = qvalues.argmax(dim=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "    \n",
    "def convert_to_tensor(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch):\n",
    "    obs_batch = torch.tensor(obs_batch)\n",
    "    act_batch = torch.tensor(act_batch)\n",
    "    reward_batch = torch.tensor(reward_batch).float()\n",
    "    next_obs_batch = torch.tensor(next_obs_batch)\n",
    "    is_done_batch = is_done_batch.astype(np.int)\n",
    "    return obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch\n",
    "  \n",
    "\n",
    "def save_data(folder_path, agent, mean_reward_history, td_loss_history):\n",
    "    torch.save(policy_agent.dqn.state_dict(), folder_path + 'atari_dqn_state_dict.pt')\n",
    "    with open(folder_path + 'mean_reward_history.l', 'wb') as f:\n",
    "        pickle.dump(mean_reward_history, f)\n",
    "    with open(folder_path + 'td_loss_history.l', 'wb') as f:\n",
    "        pickle.dump(td_loss_history, f)\n",
    "        \n",
    "\n",
    "def load_data(folder_path):\n",
    "    state_dict = None\n",
    "    mean_reward_history = []\n",
    "    td_loss_history = []\n",
    "    \n",
    "    state_dict = torch.load(folder_path + 'atari_dqn_state_dict.pt')\n",
    "    with open(folder_path + 'mean_reward_history.l', 'rb') as f:\n",
    "        mean_reward_history = pickle.load(f)\n",
    "    with open(folder_path + 'td_loss_history.l', 'rb') as f:\n",
    "        td_loss_history = pickle.load(f)\n",
    "        \n",
    "    return state_dict, mean_reward_history, td_loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIKLZZZ8W0yT"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, span, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
    "%matplotlib inline\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "\n",
    "gamma = 0.99\n",
    "policy_agent = DQNAgent(state_dim, n_actions, epsilon=0.2)\n",
    "target_agent = DQNAgent(state_dim, n_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPrey4RYd7Bm"
   },
   "outputs": [],
   "source": [
    "rl_path = '/content/gdrive/My Drive/AI/Reinforcement_Learning/atari_dqn/new_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBhy47wlIwIL"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "LuR7OSygcuLz",
    "outputId": "ffcc58ec-1dfc-4e58-a340-f20202ced9fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (l1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict, mean_rw_history, td_loss_history = load_data(rl_path)\n",
    "policy_agent.dqn.load_state_dict(state_dict)\n",
    "policy_agent.dqn.eval()\n",
    "target_agent.dqn.load_state_dict(state_dict)\n",
    "target_agent.dqn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6IiU_7NI0BK"
   },
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GetAwuFDx_qb"
   },
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10**5)\n",
    "play_and_record(policy_agent, env, exp_replay, n_steps=10000)\n",
    "optimizer = optim.Adam(policy_agent.dqn.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ye_xNTYCW0yh",
    "outputId": "807f712b-2b0a-49d9-ba17-10d748d6adc7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer size = 100000, epsilon = 0.11979\n",
      "Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "for i in trange(int(4e4)):\n",
    "    \n",
    "    # play\n",
    "    play_and_record(policy_agent, env, exp_replay, 10)\n",
    "    \n",
    "    # train\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = \\\n",
    "                     convert_to_tensor(*exp_replay.sample(batch_size=64))\n",
    "    \n",
    "    q_values = policy_agent.get_qvalues(obs_batch)\n",
    "    current_action_qvalues = policy_agent.get_qvalues_for_actions(q_values, act_batch)\n",
    "    loss_t = optimize(current_action_qvalues, optimizer, \n",
    "                      target_agent, reward_batch, next_obs_batch, is_done_batch)\n",
    "    td_loss_history.append(loss_t)\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    if i % 500 == 0:\n",
    "        target_agent.dqn.load_state_dict(policy_agent.dqn.state_dict())\n",
    "        policy_agent.epsilon = max(policy_agent.epsilon * 0.99, 0.01)\n",
    "        mean_rw_history.append(evaluate(make_env(), policy_agent, n_games=3))\n",
    "        save_data(rl_path, policy_agent, mean_rw_history, td_loss_history)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), policy_agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[48, 4])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(loss_t)\n",
    "        plt.figure(figsize=[48, 4])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(moving_average(np.array(td_loss_history), span=100, min_periods=100))\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "D14FeN6cW0yp",
    "outputId": "d85830e3-bb4e-4438-92cc-04a7eeb505d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's good enough for tutorial.\n"
     ]
    }
   ],
   "source": [
    "assert np.mean(mean_rw_history[-10:]) > 10.\n",
    "print(\"That's good enough for tutorial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPr5yHpKW0yu"
   },
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skMmv8pvW0yw"
   },
   "outputs": [],
   "source": [
    "policy_agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "np6-cyPIW0y1",
    "outputId": "8a376c7b-c3e1-411e-fa56-451611e9d65f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(make_env(),directory=\"videos\",force=True)\n",
    "sessions = [evaluate(env_monitor, policy_agent, n_games=1) for _ in range(100)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJRuyrSQW0y_"
   },
   "source": [
    "## More\n",
    "\n",
    "If you want to play with DQN a bit more, here's a list of things you can try with it:\n",
    "\n",
    "### Easy:\n",
    "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
    "  * You will probably need `tf.argmax` to select best actions\n",
    "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
    "  * You will need to change network architecture, namely the q-values layer\n",
    "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
    "  * You should then add them up via elemwise sum layer.\n",
    "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8rcf1j1W0zB"
   },
   "source": [
    "### Hard: Prioritized experience replay\n",
    "\n",
    "In this section, you're invited to implement prioritized experience replay\n",
    "\n",
    "* You will probably need to provide a custom data structure\n",
    "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
    "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
    "\n",
    "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
    "\n",
    "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
    "\n",
    "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
    "\n",
    "There is even more out there - see this [overview article](https://arxiv.org/abs/1710.02298)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dqn_atari_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
