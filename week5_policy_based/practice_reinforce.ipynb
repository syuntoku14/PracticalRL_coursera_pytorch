{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f710a7d5a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.14</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r.cumsum()[::-1].copy()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(_agent, t_max=1000):\n",
    "    s_0 = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "    action_space = np.arange(env.action_space.n)\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Get actions and convert to numpy array\n",
    "        action_probs = _agent.predict(s_0).detach().numpy()\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        s_1, r, done, _ = env.step(action)\n",
    "\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        s_0 = s_1\n",
    "           \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states, actions, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(_agent, _optimizer, states, actions, rewards, gamma=0.99):\n",
    "    _optimizer.zero_grad()\n",
    "    state_tensor = torch.FloatTensor(states)\n",
    "    reward_tensor = torch.FloatTensor(discount_rewards(rewards))\n",
    "    # Actions are used as indices, must be LongTensor\n",
    "    action_tensor = torch.LongTensor(actions)\n",
    "\n",
    "    # Calculate loss\n",
    "    prob = _agent.predict(state_tensor)\n",
    "    logprob = torch.log(\n",
    "        _agent.predict(state_tensor))\n",
    "    selected_logprobs = logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "    selected_probs = prob[np.arange(len(action_tensor)), action_tensor]\n",
    "    entropy = - torch.sum(selected_probs * selected_logprobs)\n",
    "    loss = -(reward_tensor * selected_logprobs).mean() - 0.001*entropy\n",
    "\n",
    "    # Calculate gradients\n",
    "    loss.backward()\n",
    "    # Apply gradients\n",
    "    _optimizer.step()\n",
    "    \n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = policy_estimator(env)\n",
    "optimizer = optim.Adam(pe.network.parameters(), \n",
    "                   lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:37.910\n",
      "mean reward:54.930\n",
      "mean reward:79.460\n",
      "mean reward:449.520\n",
      "mean reward:737.270\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\n",
    "    rewards = [train_on_session(pe, optimizer, *generate_session(pe))\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(_agent, t_max=1000):\n",
    "    s_0 = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "    action_space = np.arange(env.action_space.n)\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Get actions and convert to numpy array\n",
    "        action_probs = _agent.predict(s_0).detach().numpy()\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        s_1, r, done, _ = env.step(action)\n",
    "\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        s_0 = s_1\n",
    "           \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session(pe) for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.9025.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, pe, \"syuntoku14@gmail.com\", \"mygnPZCi4gI87AfJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
